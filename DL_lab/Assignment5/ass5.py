# -*- coding: utf-8 -*-
"""BatchNormalization_Regularization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QxX4D9L-mnxdALyHyudSXe8tBXeIS4oz

- Saurabh Jaiswal
- 24AI60R46

#Assignment 5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import requests
import io
import zipfile
from urllib.request import urlopen
from io import BytesIO

print("Set random seeds for reproducibility")
np.random.seed(42)
torch.manual_seed(42)

"""## Question 1 [1 mark]

"""

print("Function to load data")
def load_landsat_data():
    print("Loading Landsat Satellite Data...")

    # URLs for the dataset files
    train_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.trn"
    test_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/satimage/sat.tst"

    train_data = pd.read_csv(train_url, sep=' ', header=None)
    test_data = pd.read_csv(test_url, sep=' ', header=None)
    print("Data Downloading complete")

    X_train = train_data.iloc[:, :-1].values
    y_train = train_data.iloc[:, -1].values
    X_test = test_data.iloc[:, :-1].values
    y_test = test_data.iloc[:, -1].values

    # Print unique class values before adjustment
    unique_classes = np.unique(np.concatenate([y_train, y_test]))
    print(f"Unique class labels before adjustment: {unique_classes}")

    print("Converting 1 indexing to 0")
    y_train = y_train - 1
    y_test = y_test - 1

    print("Printing unique class values after adjustment")
    unique_classes_after = np.unique(np.concatenate([y_train, y_test]))
    print(f"Unique class : {unique_classes_after}")
    print(f"Min class label: {np.min(y_train)}, Max class label: {np.max(y_train)}")

    # Check the number of classes
    num_classes = np.max(unique_classes_after) + 1
    print(f"Number of classes: {num_classes}")

    print(f"Data loaded: {X_train.shape[0]} training samples, {X_test.shape[0]} test samples")
    print(f"Number of features: {X_train.shape[1]}")

    return X_train, y_train, X_test, y_test, num_classes
print("Data loading function complete")

print("Calling function to Load data")
X_train, y_train, X_test, y_test, num_classes = load_landsat_data()
print("Data loading complete.")

"""## Question 2 (1 mark)
Check the dataset for missing and duplicate
"""

print("Check for missing and duplicate entries and analyze class distribution ")
def analyze_data(X_train, y_train, X_test, y_test):
    print("Analyzing dataset.")

    # Combine data for analysis
    train_df = pd.DataFrame(X_train)
    train_df['target'] = y_train

    test_df = pd.DataFrame(X_test)
    test_df['target'] = y_test

    # Check for missing values
    train_missing = train_df.isnull().sum().sum()
    test_missing = test_df.isnull().sum().sum()
    print(f"Missing values: {train_missing} in training, {test_missing} in test")

    # Check for duplicates
    train_duplicates = train_df.duplicated().sum()
    test_duplicates = test_df.duplicated().sum()
    print(f"Duplicate rows: {train_duplicates} in training, {test_duplicates} in test")

    # Analyze class distribution
    train_class_counts = pd.Series(y_train).value_counts().sort_index()
    test_class_counts = pd.Series(y_test).value_counts().sort_index()

    print("Class distribution in training set:")
    for i, count in enumerate(train_class_counts):
        percentage = 100 * count / len(y_train)
        print(f"Class {i}: {count} samples ({percentage:.2f}%)")

    print("Class distribution in test set:")
    for i, count in enumerate(test_class_counts):
        percentage = 100 * count / len(y_test)
        print(f"Class {i}: {count} samples ({percentage:.2f}%)")

    # Determine if dataset is balanced
    train_percentages = [100 * count / len(y_train) for count in train_class_counts]
    max_percentage = max(train_percentages)
    min_percentage = min(train_percentages)
    imbalance_ratio = max_percentage / min_percentage

    if imbalance_ratio > 1.5:
        balance_status = "imbalanced"
    else:
        balance_status = "relatively balanced"

    print(f"Imbalance ratio (max/min class frequency): {imbalance_ratio:.2f}")
    print(f"The dataset is {balance_status}")

    # Plot class distribution
    plt.figure(figsize=(10, 6))

    plt.subplot(1, 2, 1)
    sns.barplot(x=train_class_counts.index, y=train_class_counts.values)
    plt.title('Training Class Distribution')
    plt.xlabel('Class')
    plt.ylabel('Count')

    plt.subplot(1, 2, 2)
    sns.barplot(x=test_class_counts.index, y=test_class_counts.values)
    plt.title('Test Class Distribution')
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.show()

    print("Class distribution plot")

    return train_df, test_df
print("Analysis function Complete.")

# Question 2: Analyze data
train_df, test_df = analyze_data(X_train, y_train, X_test, y_test)

print("Preprocess data")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("Data preprocessing complete.")

print("Split training data into train and validation sets")
X_train_final, X_val, y_train_final, y_val = train_test_split(
    X_train_scaled, y_train, test_size=0.2, random_state=42, stratify=y_train
)
print("Data split complete.")

"""## Question 3 (3 marks)
Design a neural network
"""

print("Custom Dataset class for PyTorch")
class LandsatDataset(Dataset):
    def __init__(self, features, labels):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.features[idx], self.labels[idx]

print("Custom BatchNormalization implementation")
class CustomBatchNorm1d(nn.Module):
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        super(CustomBatchNorm1d, self).__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum

        # Parameters to be learned
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))

        # Running statistics
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))

        self.training = True

    def forward(self, x):
        if self.training:
            # Calculate batch statistics
            batch_mean = x.mean(0)
            batch_var = x.var(0, unbiased=False)

            # Update running statistics
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.detach()
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.detach()

            # Normalize
            x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.eps)
        else:
            # Use running statistics
            x_normalized = (x - self.running_mean) / torch.sqrt(self.running_var + self.eps)

        # Scale and shift
        return self.gamma * x_normalized + self.beta

print("Create datasets and dataloaders")
train_dataset = LandsatDataset(X_train_final, y_train_final)
val_dataset = LandsatDataset(X_val, y_val)
test_dataset = LandsatDataset(X_test_scaled, y_test)

print("Defining batch size too")
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

print("Neural Network with Custom BatchNorm")
class NeuralNetworkCustomBN(nn.Module):
    def __init__(self, input_size, num_classes, l1_lambda=0.0001):
        super(NeuralNetworkCustomBN, self).__init__()
        self.l1_lambda = l1_lambda

        # Layer 1
        self.fc1 = nn.Linear(input_size, 128)
        self.bn1 = CustomBatchNorm1d(128)
        self.relu1 = nn.ReLU()

        # Layer 2
        self.fc2 = nn.Linear(128, 64)
        self.bn2 = CustomBatchNorm1d(64)
        self.relu2 = nn.ReLU()

        # Layer 3
        self.fc3 = nn.Linear(64, 32)
        self.bn3 = CustomBatchNorm1d(32)
        self.relu3 = nn.ReLU()

        # Layer 4
        self.fc4 = nn.Linear(32, 16)
        self.bn4 = CustomBatchNorm1d(16)
        self.relu4 = nn.ReLU()

        # Output layer
        self.fc5 = nn.Linear(16, num_classes)

    def forward(self, x):
        # Layer 1
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu1(x)

        # Layer 2
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.relu2(x)

        # Layer 3
        x = self.fc3(x)
        x = self.bn3(x)
        x = self.relu3(x)

        # Layer 4
        x = self.fc4(x)
        x = self.bn4(x)
        x = self.relu4(x)

        # Output layer
        x = self.fc5(x)

        return x

    def l1_regularization(self):
        l1_loss = 0
        for param in self.parameters():
            l1_loss += torch.sum(torch.abs(param))
        return self.l1_lambda * l1_loss
print("Model delaration complete.")

"""# Question 4 (5 marks)"""

print("Neural Network with PyTorch BatchNorm ")
class NeuralNetworkPyTorchBN(nn.Module):
    def __init__(self, input_size, num_classes, l1_lambda=0.0001):
        super(NeuralNetworkPyTorchBN, self).__init__()
        self.l1_lambda = l1_lambda

        # Layer 1
        self.fc1 = nn.Linear(input_size, 128)
        self.bn1 = nn.BatchNorm1d(128)
        self.relu1 = nn.ReLU()

        # Layer 2
        self.fc2 = nn.Linear(128, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.relu2 = nn.ReLU()

        # Layer 3
        self.fc3 = nn.Linear(64, 32)
        self.bn3 = nn.BatchNorm1d(32)
        self.relu3 = nn.ReLU()

        # Layer 4
        self.fc4 = nn.Linear(32, 16)
        self.bn4 = nn.BatchNorm1d(16)
        self.relu4 = nn.ReLU()

        # Output layer
        self.fc5 = nn.Linear(16, num_classes)

    def forward(self, x):
        # Layer 1
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu1(x)

        # Layer 2
        x = self.fc2(x)
        x = self.bn2(x)
        x = self.relu2(x)

        # Layer 3
        x = self.fc3(x)
        x = self.bn3(x)
        x = self.relu3(x)

        # Layer 4
        x = self.fc4(x)
        x = self.bn4(x)
        x = self.relu4(x)

        # Output layer
        x = self.fc5(x)

        return x

    def l1_regularization(self):
        l1_loss = 0
        for param in self.parameters():
            l1_loss += torch.sum(torch.abs(param))
        return self.l1_lambda * l1_loss
print("Model Declaration Complete")

print("Training function with early stopping")
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, patience=10, model_name="model"):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using {device} device")

    model.to(device)

    train_losses = []
    val_losses = []

    best_val_loss = float('inf')
    best_model_state = None
    patience_counter = 0

    for epoch in range(num_epochs):
        # Training phase
        model.train()
        running_loss = 0.0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels) + model.l1_regularization()

            # Backward pass and optimize
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)

        # Validation phase
        model.eval()
        running_loss = 0.0

        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels) + model.l1_regularization()

                running_loss += loss.item() * inputs.size(0)

        epoch_val_loss = running_loss / len(val_loader.dataset)
        val_losses.append(epoch_val_loss)

        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')

        # Check for early stopping
        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            best_model_state = model.state_dict().copy()
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f'Early stopping triggered after {epoch+1} epochs')
                break

    # Load the best model
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    return train_losses, val_losses
print("Function end")

print("Evaluation function")
def evaluate_model(model, test_loader, criterion, num_classes, model_name="model"):
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    test_loss = 0.0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels) + model.l1_regularization()

            test_loss += loss.item() * inputs.size(0)

            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    test_loss = test_loss / len(test_loader.dataset)

    # Calculate metrics
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    cm = confusion_matrix(all_labels, all_preds)

    print(f"\n{model_name} Evaluation:")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score (Weighted): {f1:.4f}")

    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(num_classes), yticklabels=range(num_classes))
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.savefig(f'{model_name}_confusion_matrix.png')
    plt.close()

    print(f"Confusion matrix saved as '{model_name}_confusion_matrix.png'")

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds))

    return accuracy, f1, cm, test_loss
print("Function complete.")

"""# Question 5 (3 marks)
Train the neural network using Adam optimizer (from PyTorch). Plot  and report the matrix.  
"""

print("Train and evaluate model")
input_size = X_train.shape[1]
model_custom_bn = NeuralNetworkCustomBN(input_size, num_classes)

print("Defining losss")
criterion = nn.CrossEntropyLoss()
print("Defuning Optimizer")
optimizer = optim.Adam(model_custom_bn.parameters(), lr=0.001)

print("Calliing train function")
custom_train_losses, custom_val_losses = train_model(
    model_custom_bn, train_loader, val_loader, criterion, optimizer,
    num_epochs=50, model_name="custom_bn"
)

print("Calling evalute function to evalute the model.")
custom_accuracy, custom_f1, custom_cm, custom_test_loss = evaluate_model(
    model_custom_bn, test_loader, criterion, num_classes, model_name="custom_bn"
)
print("Evalution Done.")

import matplotlib.pyplot as plt

print("Defining Plot function")
def plot_training_validation_loss(train_losses, val_losses, model_name):
    # Plot training and validation loss
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'Training and Validation Loss for {model_name}')
    plt.legend()
    plt.grid(True)
    plt.show()
print("plot function end")

print("Calling plot function to plot the data.")
plot_training_validation_loss(custom_train_losses, custom_val_losses, "Custom BatchNorm Model")

"""# Question 6 (3 marks)
Use PyTorch implementation
"""

print("Traing and evaluating pythorch implementation")
model_pytorch_bn = NeuralNetworkPyTorchBN(input_size, num_classes)

print("Defing Optimizer")
optimizer = optim.Adam(model_pytorch_bn.parameters(), lr=0.001)

print("Performing Traing")
pytorch_train_losses, pytorch_val_losses = train_model(
    model_pytorch_bn, train_loader, val_loader, criterion, optimizer,
    num_epochs=50, model_name="pytorch_bn"
)

print("Evaluate PyTorch BN model")
pytorch_accuracy, pytorch_f1, pytorch_cm, pytorch_test_loss = evaluate_model(
    model_pytorch_bn, test_loader, criterion, num_classes, model_name="pytorch_bn"
)
print("Evaluation done.")

print("Ploting the loss")
plot_training_validation_loss(pytorch_train_losses, pytorch_val_losses, "PyTorch BatchNorm Model")

"""#Question 7 (3 marks)
Show the corresponding loss plots, and compare the performance
"""

print("Plot comparative loss")
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(custom_train_losses, label='Custom BN Training')
plt.plot(custom_val_losses, label='Custom BN Validation')
plt.plot(pytorch_train_losses, label='PyTorch BN Training')
plt.plot(pytorch_val_losses, label='PyTorch BN Validation')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Comparison')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
comparison_data = {
    'Accuracy': [custom_accuracy, pytorch_accuracy],
    'F1 Score': [custom_f1, pytorch_f1],
    'Test Loss': [custom_test_loss, pytorch_test_loss]
}

x = np.arange(len(comparison_data))
width = 0.35

plt.bar(x - width/2, [custom_accuracy, custom_f1, custom_test_loss], width, label='Custom BN')
plt.bar(x + width/2, [pytorch_accuracy, pytorch_f1, pytorch_test_loss], width, label='PyTorch BN')

plt.xlabel('Metrics')
plt.ylabel('Values')
plt.title('Performance Metrics Comparison')
plt.xticks(x, comparison_data.keys())
plt.legend()
plt.grid(True)
print("Plotting Done")

print("\nPerformance Comparison Summary:")
print(f"Custom BN - Accuracy: {custom_accuracy:.4f}, F1: {custom_f1:.4f}, Test Loss: {custom_test_loss:.4f}")
print(f"PyTorch BN - Accuracy: {pytorch_accuracy:.4f}, F1: {pytorch_f1:.4f}, Test Loss: {pytorch_test_loss:.4f}")

diff_accuracy = (pytorch_accuracy - custom_accuracy) * 100
diff_f1 = (pytorch_f1 - custom_f1) * 100
diff_loss = (custom_test_loss - pytorch_test_loss) / custom_test_loss * 100
print(f"\nPyTorch BN vs Custom BN:")
print(f"Accuracy difference: {diff_accuracy:.2f}% {'better' if diff_accuracy > 0 else 'worse'}")
print(f"F1 Score difference: {diff_f1:.2f}% {'better' if diff_f1 > 0 else 'worse'}")
print(f"Test Loss improvement: {diff_loss:.2f}% {'reduction' if diff_loss > 0 else 'increase'}")

conclusion = """
\nConclusion:
The comparison between custom-implemented batch normalization and PyTorch's built-in batch normalization shows:
"""
if abs(diff_accuracy) < 1 and abs(diff_f1) < 1 and abs(diff_loss) < 5:
    conclusion += "Both implementations perform similarly, indicating that our custom batch normalization is properly implemented."
elif diff_accuracy > 0 and diff_f1 > 0 and diff_loss > 0:
    conclusion += "PyTorch's implementation outperforms our custom implementation, which is expected due to optimizations in the PyTorch library."
else:
    conclusion += "The performance differences are mixed, suggesting that while both implementations work, there may be nuances in how they handle this specific dataset."

print(conclusion)

