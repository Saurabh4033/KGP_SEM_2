# -*- coding: utf-8 -*-
"""Perceptron_assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvQQUNp976SK31CqIrtoZbbLTvFXnswX

# Assignment 2

- Saurabh Jaiswal
- 20CS3058

## Question 1: (2 Marks)
Load the digits dataset from sklearn
"""

from sklearn.datasets import load_digits
def dataset():
  print("Loading dataset")
  digits = load_digits()
  print(digits.data.shape)
  import matplotlib.pyplot as plt
  plt.gray()
  plt.matshow(digits.images[0])
  plt.show()
  return digits

print("Calling dataset to get the data")
digits=dataset()

"""## Question 2: (2 Marks)
Split the dataset into training and test sets.  
"""

print(digits.data.shape)

#split the dataset
from sklearn.model_selection import train_test_split
print("Splitting data")
def split_data(digits):
  X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)
  return  X_train, X_test, y_train, y_test

print("Calling split data")
X_train, X_test, y_train, y_test=split_data(digits)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""## Question 3: (6 Marks)
Implement the perceptron algorithm from scratch and fit the perceptron model on the training set  
"""

import numpy as np
from tqdm import tqdm
from sklearn.metrics import accuracy_score

print("Custom Model defination.")
class CustomPerceptron:
    def __init__(self, num_features: int, lr: float = 0.1):
        self.W = np.zeros(num_features + 1)  # bias
        self.lr = lr

    def activation_function(self, x):
        return 1 if x >= 0 else -1

    def loss_fn(self, pred, target):
        return target * pred

    def forward(self, x):
        x = np.append(x, [1])  #bias term added
        value = np.dot(self.W, x)
        out = self.activation_function(value)
        return out, value

    def weight_update(self, error, input, target):
        if error < 0:
            input = np.append(input, [1])  #bias term added
            self.W += self.lr * target * input
print("Custom Model defination End.")

print("Perceptrons for Multi-Class Classification")
def train_perceptrons(X_train, y_train, lr=0.01, max_epochs=500):
    num_classes = len(np.unique(y_train))
    num_features = X_train.shape[1]
    print("Training Start.")
    print(f"Number of classes: {num_classes}")
    print(f"Number of features: {num_features}")
    perceptrons = [CustomPerceptron(num_features, lr) for _ in range(num_classes)]
    misclassified_samples = []
    # Train each perceptron
    for class_index in range(num_classes):
        model = perceptrons[class_index]
        print(f"Training Perceptron {class_index}")
        misclassified = []
        for epoch in tqdm(range(max_epochs)):
            num_misclassifications = 0

            for i in range(len(X_train)):
                # Convert multi-class target into binary for this class
                target = 1 if y_train[i] == class_index else -1
                pred, _ = model.forward(X_train[i])

                # Calculate error
                error = model.loss_fn(pred, target)
                if error < 0:
                    num_misclassifications += 1
                    model.weight_update(error, X_train[i], target)
            misclassified.append(num_misclassifications)

            if num_misclassifications == 0:
                print(f"No misclassifications. Stopping early at epoch {epoch + 1}.")
                break
        misclassified_samples.append(misclassified)
    return perceptrons, misclassified_samples
    print("Training End.")

"""## Question 4: (2 Marks)
How many perceptron classifiers must be trained for this problem?

- As it is multiclass classification with the perceptron, one-vs-all  is used. Since there are 10 classes (digits 0–9), 10 perceptron classifiers must be trained, one for each class.

## Question 5: (2 Marks)
Show the variation of the number of misclassified examples with the number of iterations of each perceptron classifier. Terminate the learning of each classifier when the number of misclassified samples stops reducing any further.
"""

# Train perceptrons
print("Training Perceptrons")
perceptrons,misclassified_samples = train_perceptrons(X_train, y_train, lr=0.01, max_epochs=500)
print("Training Perceptrons End.")

import numpy as np
import matplotlib.pyplot as plt

print("Plotting misclassifications.")
def plot_misclassifications(misclassified_samples):
    max_epochs = max(len(samples) for samples in misclassified_samples)

    for digit in range(len(misclassified_samples)):
        current_length = len(misclassified_samples[digit])
        if current_length < max_epochs:
            last_value = misclassified_samples[digit][-1]
            misclassified_samples[digit].extend([last_value] * (max_epochs - current_length))

    plt.figure(figsize=(10, 6))
    digit_colors = plt.cm.get_cmap("tab10", len(misclassified_samples))  #distinct colors for digits

    for digit, samples in enumerate(misclassified_samples):
        plt.plot(
            range(1, len(samples) + 1),
            samples,
            label=f"Digit {digit}",
            color=digit_colors(digit)
        )

    plt.xlabel('Epochs')
    plt.ylabel('Number of Misclassified Samples')
    plt.title('Misclassifications per Epoch for Each Digit')
    plt.legend(title="Digits", loc="upper right")
    plt.show()
print("Misclassification function End.")

print("Calling plot function.")
plot_misclassifications(misclassified_samples)

"""## Question 6: (2 marks)
Generate the predictions on the test set, and report the accuracy, precision and recall performances.  
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score
print("Defining test functions.")
def test_perceptrons(perceptrons, X_test, y_test):
    predictions = []

    for x in X_test:
        outputs = []
        for perceptron in perceptrons:
            _, value = perceptron.forward(x)
            outputs.append(value)
        predicted_class = np.argmax(outputs)
        predictions.append(predicted_class)
    accuracy = accuracy_score(y_test, predictions)
    precision = precision_score(y_test, predictions, average='macro')
    recall = recall_score(y_test, predictions, average='macro')
    # print(f"Test Accuracy: {accuracy * 100:.2f}%")
    return accuracy,precision,recall,predictions

custom_accuracy,custom_precision,custom_recall,custom_predictions=test_perceptrons(perceptrons, X_test, y_test)
print("Accuracy:", custom_accuracy)
print("Precision:", custom_precision)
print("Recall:", custom_recall)

"""## Question 7: (3 Marks)
Generate the class predictions with sklearn’s implementation of perceptron. Compare the corresponding performance metrics (accuracy, precision, recall) with your implementation.  
"""

from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report

print("Defining sklearn perceptrons.")
def test_sklearn_perceptrons(X_train,X_test,y_train):
  # Initialize and train Perceptron
  clf = Perceptron(max_iter=500, tol=1e-3, random_state=42)
  clf.fit(X_train, y_train)

  # Make predictions
  sklearn_predictions = clf.predict(X_test)
  return sklearn_predictions
print("Sklearn Perceptron end.")

# Generate predictions
print("Generating sklearn predictions.")
sklearn_predictions = test_sklearn_perceptrons(X_train,X_test,y_train)

# Metrics for sklearn Perceptron
print("Calculating sklearn metrics.")
sklearn_accuracy = accuracy_score(y_test, sklearn_predictions)
sklearn_precision = precision_score(y_test, sklearn_predictions, average='macro')
sklearn_recall = recall_score(y_test, sklearn_predictions, average='macro')

print("Printing Metrics.")
def print_metrics(custom_accuracy,custom_precision,custom_recall,sklearn_accuracy,sklearn_precision,sklearn_recall):
    print("\n--- Performance Metrics ---")
    print("Custom Perceptron:")
    print(f"Accuracy: {custom_accuracy * 100:.2f}%")
    print(f"Precision : {custom_precision:.2f}")
    print(f"Recall : {custom_recall:.2f}")
    print("\nSklearn Perceptron:")
    print(f"Accuracy: {sklearn_accuracy * 100:.2f}%")
    print(f"Precision : {sklearn_precision:.2f}")
    print(f"Recall : {sklearn_recall:.2f}")

print("Calling print metrics.")
print_metrics(custom_accuracy,custom_precision,custom_recall,sklearn_accuracy,sklearn_precision,sklearn_recall)

print("\n--- Classification Report ---")
print("Custom Perceptron:\n", classification_report(y_test, custom_predictions))
print("Sklearn Perceptron:\n", classification_report(y_test, sklearn_predictions))

"""## Question 8: (1 Marks)
From the performances obtained, what can you conclude about the separability of the dataset?

- The performance metrics indicate that the dataset is not perfectly linearly separable. Perceptron struggles with non-linearly separable datasets, but digit 0 may still be separable from the rest due to its distinct features. For better performance, advanced algorithms like SVMs or neural networks can be used.
"""