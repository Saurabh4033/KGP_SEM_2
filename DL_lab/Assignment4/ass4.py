# -*- coding: utf-8 -*-
"""Adadelta_As4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WEiaiZ-m0Sv0cHgsh0wFlNTWNjZZhTL9

- Saurabh Jaiswal
- 24AI60R46

## Question 1 (1 mark)

Load the Forest Cover Types dataset using the sklearn library. Split the dataset into training, validation, and test sets.
"""

print("Import required libraries")
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, f1_score

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
print("Import done")

print("Set random seeds for reproducibility")
torch.manual_seed(42)
np.random.seed(42)
print("random initialized")

print("Load the Forest Cover Types dataset")
def load_data():
  data = fetch_openml('covertype', version=1, parser='auto')
  X, y = data.data, data.target

  # Preprocess and split
  scaler = StandardScaler()
  X = scaler.fit_transform(X)

  # Convert target labels to numerical using LabelEncoder
  label_encoder = LabelEncoder()
  y = label_encoder.fit_transform(y)

  # Split into train/val/test (60/20/20)
  X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)
  X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

  # Convert to PyTorch tensors
  train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                              torch.tensor(y_train, dtype=torch.long))
  val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),
                            torch.tensor(y_val, dtype=torch.long))
  test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32),
                              torch.tensor(y_test, dtype=torch.long))

  # Create DataLoaders
  batch_size = 256
  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
  val_loader = DataLoader(val_dataset, batch_size=batch_size)
  test_loader = DataLoader(test_dataset, batch_size=batch_size)

  return train_loader, val_loader, test_loader,X_train, X_val, X_test, y_train, y_val, y_test,X,y

print("Dataloader def done")

print("calling the load data to load the data")
train_loader, val_loader, test_loader,X_train, X_val, X_test, y_train, y_val, y_test,X,y = load_data()
print("data loaded")

"""## Question 2 (3 marks)

Design a fully connected neural network for classifying the forest cover types. Use 2 hidden layers with 128 and 64 neurons repectively
"""

print("Design a fully connected neural network for classifying the forest cover types")
class ForestCoverNN(nn.Module):
    def __init__(self, input_size, num_classes):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        return self.net(x)
print("done")

print("Initialize model")
input_size = X_train.shape[1]
num_classes = len(np.unique(y))
model = ForestCoverNN(input_size, num_classes)
print("done")

"""## Question 3 (5 marks)

Implement Nesterov Accelerated Gradient Descent (NAG) from scratch to train the designed neural network. Train the model on the training set and plot the training and validation loss curves. Explain the observed trends in the loss curves.
"""

print("Implement Nesterov Accelerated Gradient Descent (NAG) from scratch to train the designed neural network")
class NAGOptimizer:
    def __init__(self, params, lr=0.01, momentum=0.9):
        self.params = list(params)
        self.lr = lr
        self.momentum = momentum
        self.velocities = [torch.zeros_like(p) for p in self.params]

    def step(self):
        with torch.no_grad():
            for i, (param, velocity) in enumerate(zip(self.params, self.velocities)):
                if param.grad is None:
                    continue

                # Update velocity
                new_velocity = self.momentum * velocity - self.lr * param.grad
                # Update parameters
                param += self.momentum * new_velocity - self.lr * param.grad
                self.velocities[i] = new_velocity

    def zero_grad(self):
        for param in self.params:
            param.grad = None
print("done")

print("Train the model on the training set and plot the training and validation loss curves")
def train_model(model, train_loader, val_loader, optimizer, epochs=50):
    criterion = nn.CrossEntropyLoss()
    train_losses, val_losses = [], []

    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        for inputs, targets in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * inputs.size(0)

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for inputs, targets in val_loader:
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                val_loss += loss.item() * inputs.size(0)

        # Calculate average losses
        train_loss = train_loss / len(train_loader.dataset)
        val_loss = val_loss / len(val_loader.dataset)
        train_losses.append(train_loss)
        val_losses.append(val_loss)

        if((epoch+1) % 10 == 0):
          print(f'Epoch {epoch+1}/{epochs} | '
                f'Train Loss: {train_loss:.4f} | '
                f'Val Loss: {val_loss:.4f}')

    return train_losses, val_losses
print("done")

print("Train with scratch NAG")
nag_model = ForestCoverNN(input_size, num_classes)
nag_optimizer = NAGOptimizer(nag_model.parameters(), lr=0.01, momentum=0.9)
nag_train_loss, nag_val_loss = train_model(nag_model, train_loader, val_loader, nag_optimizer)

print("Defining plot function.")
def plot_loss_curves(train_losses, val_losses, title):
    plt.plot(train_losses, label='Training Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title(title)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
print("Done")

print("Calling fun plot to plot the function.")
plot_loss_curves(nag_train_loss, nag_val_loss, 'NAG Training (Scratch Implementation)')

"""## Question 4 (3 marks)

Train the same neural network using PyTorch’s SGD with Nesterov Accelerated Gradient Descent. Compare the training and validation loss curves obtained from the scratch implementation and PyTorch’s implementation. Additionally, evaluate and compare the performance of both implementations on the test set in terms of accuracy and F1-score.
"""

print("Train with PyTorch's NAG")
pytorch_nag_model = ForestCoverNN(input_size, num_classes)
optimizer = torch.optim.SGD(pytorch_nag_model.parameters(), lr=0.01, momentum=0.9, nesterov=True)
pytorch_train_loss, pytorch_val_loss = train_model(pytorch_nag_model, train_loader, val_loader, optimizer)

print("Plotting comparison")
def plot_comparison(train_losses, val_losses, pytorch_train_losses, pytorch_val_losses, title):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Scratch NAG Train')
    plt.plot(val_losses, label='Scratch NAG Val')
    plt.plot(pytorch_train_losses, '--', label='PyTorch NAG Train')
    plt.plot(pytorch_val_losses, '--', label='PyTorch NAG Val')
    plt.title(title)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
print("Done")

print("Calling plot function to plot")
plot_comparison(nag_train_loss, nag_val_loss, pytorch_train_loss, pytorch_val_loss, 'NAG Implementations Comparison')

print("Evaluate models")
def evaluate_model(model, loader):
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for inputs, targets in loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(targets.numpy())
            y_pred.extend(predicted.numpy())
    return accuracy_score(y_true, y_pred), f1_score(y_true, y_pred, average='weighted')

print("Evaluate both implementations")
scratch_acc, scratch_f1 = evaluate_model(nag_model, test_loader)
pytorch_acc, pytorch_f1 = evaluate_model(pytorch_nag_model, test_loader)

print(f"\nScratch NAG Results: Accuracy={scratch_acc:.4f}, F1-score={scratch_f1:.4f}")
print(f"PyTorch NAG Results: Accuracy={pytorch_acc:.4f}, F1-score={pytorch_f1:.4f}")

"""## Question 5 (5 marks)

Implement Adadelta from scratch to train the neural network and plot the training and validation loss curves.
"""

print("Implement Adadelta from scratch to train the neural network")
class AdadeltaOptimizer:
    def __init__(self, params, rho=0.9, eps=1e-6):
        self.params = list(params)
        self.rho = rho
        self.eps = eps
        self.acc_grad = [torch.zeros_like(p) for p in self.params]
        self.acc_update = [torch.zeros_like(p) for p in self.params]

    def step(self):
        with torch.no_grad():
            for i, param in enumerate(self.params):
                if param.grad is None:
                    continue

                # Update accumulated gradient
                self.acc_grad[i] = self.rho * self.acc_grad[i] + (1 - self.rho) * param.grad**2

                # Compute update
                update = - (torch.sqrt(self.acc_update[i] + self.eps) /
                           torch.sqrt(self.acc_grad[i] + self.eps)) * param.grad

                # Update parameters
                param += update

                # Update accumulated updates
                self.acc_update[i] = self.rho * self.acc_update[i] + (1 - self.rho) * update**2

    def zero_grad(self):
        for param in self.params:
            param.grad = None
print("Done")

print("Train with scratch Adadelta")
adadelta_model = ForestCoverNN(input_size, num_classes)
adadelta_optimizer = AdadeltaOptimizer(adadelta_model.parameters())
adadelta_train_loss, adadelta_val_loss = train_model(adadelta_model, train_loader, val_loader, adadelta_optimizer)

print("Plot the training and validation loss curves")
plot_loss_curves(adadelta_train_loss, adadelta_val_loss, 'Adadelta Training (Scratch Implementation)')

"""## Question 6 (3 marks)

Train the same model using PyTorch’s Adadelta optimizer and compare the training and validation loss curves. Evaluate both implementations on the test set in terms of accuracy and F1-score, and provide a detailed comparison of the performance.
"""

print("Train with PyTorch's Adadelta")
pytorch_adadelta_model = ForestCoverNN(input_size, num_classes)
optimizer = torch.optim.Adadelta(pytorch_adadelta_model.parameters())
pytorch_ad_train, pytorch_ad_val = train_model(pytorch_adadelta_model, train_loader, val_loader, optimizer)

print("Plotting comparison")
plot_comparison(adadelta_train_loss, adadelta_val_loss, pytorch_ad_train, pytorch_ad_val, 'Adadelta Implementations Comparison')

print("Evaluate both implementations")
scratch_acc, scratch_f1 = evaluate_model(adadelta_model, test_loader)
pytorch_acc, pytorch_f1 = evaluate_model(pytorch_adadelta_model, test_loader)

print(f"\nScratch Adadelta Results: Accuracy={scratch_acc:.4f}, F1-score={scratch_f1:.4f}")
print(f"PyTorch Adadelta Results: Accuracy={pytorch_acc:.4f}, F1-score={pytorch_f1:.4f}")

